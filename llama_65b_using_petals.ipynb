{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0twkRnKXo_GP"
      },
      "outputs": [],
      "source": [
        "# %pip install git+https://github.com/bigscience-workshop/petals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from petals import AutoDistributedModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"enoch/llama-65b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False, add_bos_token=False)\n",
        "\n",
        "model = AutoDistributedModelForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYmj92ffpLv-",
        "outputId": "f29b0077-cd44-422e-cab8-a72284310d84"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Jul 16 12:56:52.620 [\u001b[1m\u001b[34mINFO\u001b[0m] LLaMA is available solely for non-commercial research purposes. Make sure you follow the terms of use: https://bit.ly/llama-license\n",
            "Jul 16 12:56:52.631 [\u001b[1m\u001b[34mINFO\u001b[0m] Using DHT prefix: llama-65b-hf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(prompt, max_new_tokens=20):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens,temperature=0.9,top_p=20,do_sample=True)\n",
        "    return tokenizer.decode(outputs[0])\n",
        "prompt = \"I am reading book,thinking about healthy life, \"\n",
        "output = generate_output(prompt)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV7eeuELqwhA",
        "outputId": "2b45b0ff-70f3-4c21-95e5-036983b7a329"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am reading book,thinking about healthy life, 3 times per-day pray God. I like to profess to you,I am a good man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NToDB8itK0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}